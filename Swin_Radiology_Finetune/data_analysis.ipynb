{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoFeatureExtractor, SwinModel\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from paths import DICT_MIMIC_OBS_TO_INT, DICT_MIMIC_INT_TO_OBS, IMAGES_MIMIC_PATH, MIMIC_PATH_TEST, MIMIC_PATH_TRAIN, MIMIC_PATH_VAL, SWINB_IMAGENET22K_WEIGHTS, DICT_MIMIC_OBSKEY_TO_INT\n",
    "\n",
    "class MIMICDataset(Dataset):\n",
    "    def __init__(self, transform, processor, partition, dataset_path, img_root_dir, label_map):\n",
    "        self.transform = transform\n",
    "        self.processor = processor\n",
    "        self.partition = partition\n",
    "        self.dataset_df = pd.read_csv(dataset_path)\n",
    "        self.img_root_dir = pathlib.Path(img_root_dir)\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #img_name = self.img_root_dir / self.dataset_df.iloc[idx].images.split(\",\")[0]\n",
    "        #img = Image.open(img_name).convert(\"RGB\")\n",
    "\n",
    "        # if isinstance(self.transform, transforms.Compose):\n",
    "        #     img = self.transform(img)\n",
    "        # elif isinstance(self.transform, A.core.composition.Compose):\n",
    "        #     img = self.transform(image=np.array(img))[\"image\"]\n",
    "        # else:\n",
    "        #     raise ValueError(\"Unknown transformation type.\")\n",
    "\n",
    "        # img = self.processor(img, return_tensors=\"pt\", size=384).pixel_values.squeeze()\n",
    "        # Right now i'm calculating class weights so I'm not using the images\n",
    "        img = torch.zeros(3, 384, 384)\n",
    "        labels = torch.tensor([self.label_map[label] for label in self.dataset_df.iloc[idx].labels.split(\",\")]).long()\n",
    "        return {\"image\": img, \"labels\": labels}\n",
    "    \n",
    "    def get_labels_name(self, label_list):\n",
    "        for i in range(len(label_list)):\n",
    "            if label_list[i] == 1:\n",
    "                print(DICT_MIMIC_INT_TO_OBS[i])\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mobrrei/miniconda3/envs/finetune/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from platform import processor\n",
    "\n",
    "\n",
    "val_train_transform = transforms.Compose([\n",
    "                transforms.Resize(416),\n",
    "                transforms.CenterCrop(384),\n",
    "                ])\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(SWINB_IMAGENET22K_WEIGHTS)\n",
    "\n",
    "train_dataset = MIMICDataset(val_train_transform, processor, \"train\", MIMIC_PATH_TRAIN, IMAGES_MIMIC_PATH, DICT_MIMIC_OBSKEY_TO_INT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample  0  : tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "sample  1  : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "sample  2  : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "sample  3  : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "sample  4  : tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0])\n",
      "sample  5  : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "sample  6  : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "sample  7  : tensor([2, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "sample  8  : tensor([2, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0])\n",
      "sample  9  : tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"sample \", i, \" :\", train_dataset[i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_class_weights(dataset, num_classes=14, num_outcomes=3):\n",
    "    \"\"\"\n",
    "    Calculate weights for each class and outcome based on their frequency in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: A PyTorch Dataset object that includes all the data.\n",
    "        num_classes: Number of classes (abnormalities).\n",
    "        num_outcomes: Number of possible outcomes for each class.\n",
    "\n",
    "    Returns:\n",
    "        class_weights: A tensor of shape (num_classes, num_outcomes) containing the weights.\n",
    "    \"\"\"\n",
    "    # Initialize frequency counters for each class and outcome\n",
    "    frequency = torch.zeros((num_classes, num_outcomes))\n",
    "    \n",
    "    # Iterate through the dataset to compute frequencies\n",
    "    for sample in tqdm(dataset, desc=\"Calculating Frequencies\", unit=\"sample\"):\n",
    "        labels = sample[\"labels\"]  # Shape: (num_classes,)\n",
    "        for i in range(num_classes):\n",
    "            frequency[i, labels[i]] += 1\n",
    "    \n",
    "    # Calculate weights: inverse of frequency, normalized for each class\n",
    "    # Calculate weights only\n",
    "    class_weights = 1 / (frequency + 1e-6)  # Add small value to avoid division by zero\n",
    "    class_weights = class_weights / class_weights.sum(dim=1, keepdim=True)  # Normalize weights per class\n",
    "\n",
    "    # Print frequency distribution for each class\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {DICT_MIMIC_INT_TO_OBS[i]}: Frequencies {frequency[i].tolist()} -> Weights {class_weights[i].tolist()}\")\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Frequencies: 100%|██████████| 152173/152173 [00:22<00:00, 6671.13sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class enlarged cardiomediastinum: Frequencies [123861.0, 8461.0, 19851.0] -> Weights [0.045706793665885925, 0.6691040396690369, 0.28518912196159363]\n",
      "Class cardiomegaly: Frequencies [109670.0, 35635.0, 6868.0] -> Weights [0.04988563433289528, 0.1535276472568512, 0.7965866923332214]\n",
      "Class lung opacity: Frequencies [110976.0, 41051.0, 146.0] -> Weights [0.0013092210283502936, 0.003539307741448283, 0.9951515197753906]\n",
      "Class lung lesion: Frequencies [145867.0, 5613.0, 693.0] -> Weights [0.004210993647575378, 0.10943257063627243, 0.8863564133644104]\n",
      "Class edema: Frequencies [130480.0, 13905.0, 7788.0] -> Weights [0.03684917092323303, 0.345780611038208, 0.6173702478408813]\n",
      "Class consolidation: Frequencies [143326.0, 5082.0, 3765.0] -> Weights [0.01486531924456358, 0.4192417860031128, 0.5658929347991943]\n",
      "Class pneumonia: Frequencies [137846.0, 4949.0, 9378.0] -> Weights [0.0229609664529562, 0.6395387649536133, 0.3375002443790436]\n",
      "Class atelectasis: Frequencies [109815.0, 35759.0, 6599.0] -> Weights [0.048280879855155945, 0.148269385099411, 0.8034497499465942]\n",
      "Class pneumothorax: Frequencies [146367.0, 4455.0, 1351.0] -> Weights [0.00703263096511364, 0.2310539036989212, 0.7619134783744812]\n",
      "Class pleural effusion: Frequencies [115555.0, 31787.0, 4831.0] -> Weights [0.03502041846513748, 0.1273094117641449, 0.8376701474189758]\n",
      "Class pleural other: Frequencies [148374.0, 3017.0, 782.0] -> Weights [0.004168127663433552, 0.20498566329479218, 0.7908462285995483]\n",
      "Class fracture: Frequencies [146143.0, 5700.0, 330.0] -> Weights [0.0021299405489116907, 0.05460980907082558, 0.9432602524757385]\n",
      "Class support devices: Frequencies [108755.0, 43399.0, 19.0] -> Weights [0.00017459769151173532, 0.0004375301650725305, 0.9993878602981567]\n",
      "Class no finding: Frequencies [118872.0, 33301.0, 0.0] -> Weights [8.41241052512709e-12, 3.002912848937278e-11, 1.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = calculate_class_weights(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def calculate_class_weights(dataset, num_classes=14, num_outcomes=3, low_weight=1e-6):\n",
    "    \"\"\"\n",
    "    Calculate weights for each class and outcome based on their frequency in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: A PyTorch Dataset object that includes all the data.\n",
    "        num_classes: Number of classes (abnormalities).\n",
    "        num_outcomes: Number of possible outcomes for each class.\n",
    "        low_weight: The weight to assign to outcomes with 0 occurrences.\n",
    "\n",
    "    Returns:\n",
    "        class_weights: A tensor of shape (num_classes, num_outcomes) containing the weights.\n",
    "    \"\"\"\n",
    "    # Initialize frequency counters for each class and outcome\n",
    "    frequency = torch.zeros((num_classes, num_outcomes))\n",
    "    \n",
    "    # Iterate through the dataset to compute frequencies\n",
    "    for sample in tqdm(dataset, desc=\"Calculating Frequencies\", unit=\"sample\"):\n",
    "        labels = sample[\"labels\"]  # Shape: (num_classes,)\n",
    "        for i in range(num_classes):\n",
    "            frequency[i, labels[i]] += 1\n",
    "    \n",
    "    # Initialize the class weights tensor\n",
    "    class_weights = torch.zeros_like(frequency)\n",
    "    \n",
    "    # Calculate weights: inverse of frequency, normalized for each class\n",
    "    for i in range(num_classes):\n",
    "        total_freq = frequency[i].sum() + low_weight * (frequency[i] == 0).sum()  # Include low_weight for 0-occurrence outcomes\n",
    "        for j in range(num_outcomes):\n",
    "            if frequency[i, j] > 0:\n",
    "                class_weights[i, j] = 1 / frequency[i, j]\n",
    "            else:\n",
    "                class_weights[i, j] = low_weight  # Assign low weight to 0-occurrence outcomes\n",
    "        class_weights[i] /= class_weights[i].sum()  # Normalize weights per class\n",
    "\n",
    "    # Print frequency distribution and weights for each class\n",
    "    for i in range(num_classes):\n",
    "        print(f\"Class {i}: Frequencies {frequency[i].tolist()} -> Weights {class_weights[i].tolist()}\")\n",
    "\n",
    "    return class_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Frequencies:   0%|          | 0/152173 [00:00<?, ?sample/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Frequencies: 100%|██████████| 152173/152173 [00:22<00:00, 6622.37sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: Frequencies [123861.0, 8461.0, 19851.0] -> Weights [0.045706793665885925, 0.6691040396690369, 0.28518912196159363]\n",
      "Class 1: Frequencies [109670.0, 35635.0, 6868.0] -> Weights [0.04988563433289528, 0.1535276472568512, 0.7965866923332214]\n",
      "Class 2: Frequencies [110976.0, 41051.0, 146.0] -> Weights [0.0013092210283502936, 0.003539307741448283, 0.9951515197753906]\n",
      "Class 3: Frequencies [145867.0, 5613.0, 693.0] -> Weights [0.004210993647575378, 0.10943257063627243, 0.8863564133644104]\n",
      "Class 4: Frequencies [130480.0, 13905.0, 7788.0] -> Weights [0.03684917092323303, 0.345780611038208, 0.6173702478408813]\n",
      "Class 5: Frequencies [143326.0, 5082.0, 3765.0] -> Weights [0.01486531924456358, 0.4192417860031128, 0.5658929347991943]\n",
      "Class 6: Frequencies [137846.0, 4949.0, 9378.0] -> Weights [0.0229609664529562, 0.6395387649536133, 0.3375002443790436]\n",
      "Class 7: Frequencies [109815.0, 35759.0, 6599.0] -> Weights [0.048280879855155945, 0.148269385099411, 0.8034497499465942]\n",
      "Class 8: Frequencies [146367.0, 4455.0, 1351.0] -> Weights [0.00703263096511364, 0.2310539036989212, 0.7619134783744812]\n",
      "Class 9: Frequencies [115555.0, 31787.0, 4831.0] -> Weights [0.03502041846513748, 0.1273094117641449, 0.8376701474189758]\n",
      "Class 10: Frequencies [148374.0, 3017.0, 782.0] -> Weights [0.004168127663433552, 0.20498566329479218, 0.7908462285995483]\n",
      "Class 11: Frequencies [146143.0, 5700.0, 330.0] -> Weights [0.0021299405489116907, 0.05460980907082558, 0.9432602524757385]\n",
      "Class 12: Frequencies [108755.0, 43399.0, 19.0] -> Weights [0.00017459766240790486, 0.0004375301068648696, 0.9993878602981567]\n",
      "Class 13: Frequencies [118872.0, 33301.0, 0.0] -> Weights [0.2132880836725235, 0.7613579034805298, 0.02535397931933403]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class_weights = calculate_class_weights(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the class weights\n",
    "torch.save(class_weights, \"class_weights.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
